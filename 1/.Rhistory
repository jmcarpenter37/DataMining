#
roc_result$auc
#
roc_result
# AUC
auc<-performance(rates, measure = "auc")
auc
# confusion matrix
table(test$crime_lev, preds>0.5)
9/164
8/164
8/(81+8)
9 / (9+155)
# confusion matrix
table(test$crime_lev, preds>0.4)
# confusion matrix
table(test$crime_lev, preds>0.6)
# confusion matrix
table(test$crime_lev, preds>0.3)
# confusion matrix
table(test$crime_lev, preds>0.5)
# confusion matrix
table(test$crime_lev, preds>0.55)
# confusion matrix
table(test$crime_lev, preds>0.48)
# confusion matrix
table(test$crime_lev, preds>0.49)
# confusion matrix
table(test$crime_lev, preds>0.5)
# confusion matrix
table(test$crime_lev, preds>0.4)
# confusion matrix
table(test$crime_lev, preds>0.4)
# confusion matrix
table(test$crime_lev, preds>0.3)
# confusion matrix
table(test$crime_lev, preds>0.1)
# confusion matrix
table(test$crime_lev, preds>0.4)
# confusion matrix
table(test$crime_lev, preds>0.6)
# confusion matrix
table(test$crime_lev, preds>0.5)
# confusion matrix
table(test$crime_lev, preds>0.3)
# confusion matrix
table(test$crime_lev, preds>0.7)
# confusion matrix
table(test$crime_lev, preds>0.6)
# confusion matrix
table(test$crime_lev, preds>0.55)
# confusion matrix
table(test$crime_lev, preds>0.53)
# confusion matrix
table(test$crime_lev, preds>0.5333333)
# confusion matrix
table(test$crime_lev, preds>0.54)
# confusion matrix
table(test$crime_lev, preds>0.545)
# confusion matrix
table(test$crime_lev, preds>0.5459)
# confusion matrix
table(test$crime_lev, preds>0.55)
# confusion matrix
table(test$crime_lev, preds>0.5)
# confusion matrix
table(test$crime_lev, preds>0.9)
# confusion matrix
table(test$crime_lev, preds>0.8)
# confusion matrix
table(test$crime_lev, preds>0.7)
# confusion matrix
table(test$crime_lev, preds>0.6)
# confusion matrix
table(test$crime_lev, preds>0.6)
# confusion matrix
table(test$crime_lev, preds>0.7)
# confusion matrix
table(test$crime_lev, preds>0.1)
# confusion matrix
table(test$crime_lev, preds>0.3)
# confusion matrix
table(test$crime_lev, preds>0.4)
# confusion matrix
table(test$crime_lev, preds>0.3)
# confusion matrix
table(test$crime_lev, preds>0.2)
# confusion matrix
table(test$crime_lev, preds>0.7)
# confusion matrix
table(test$crime_lev, preds>0.5)
qnorm(28.78)
qt(28.78)
pt(28.78)
qnorm(2)
pnorm(28.72)
pnorm(28.78)
pnorm(1.6427)
1-pnorm(1.6427)
library(MASS)
attach(Boston)
#
Boston
#
Boston$crime_lev <- ifelse(crim < 1 , "low" , "high")
Boston$crime_lev <- as.factor(Boston$crime_lev)
contrasts(Boston$crime_lev)
# b
set.seed(199)
sample<-sample.int(nrow(Boston), floor(.50*nrow(Boston)), replace = F)
train<-Boston[sample, ]
test<-Boston[-sample, ]
# Fit the model
result <- glm(crime_lev ~ indus , family="binomial",data=train)
result
# plot rocr
library(ROCR)
preds<-predict(result,newdata=test, type="response")
rates<-prediction(preds, test$crime_lev)
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result)
lines(x = c(0,1), y = c(0,1), col="red")
# AUC
auc<-performance(rates, measure = "auc")
auc
# confusion matrix
table(test$crime_lev, preds>0.5)
ll
library(ISLR)
library(College)
install.packages("ISLR")
library(ISLR)
data <- attach(College)
data
attach(Colleg)
attach(College)
College
model <- lm(Accept ~ . , data = College)
model
summary(model)
plot(model$fitted.values , model$residuals)
plot(College)
pairs(College)
library(glmnet)
##model.matrix automatically transform categorical variables into dummy codes, which is needed as the glmnet function cannot handle categorical variables
x<-model.matrix(College~.,data =  College)[,-1]
##model.matrix automatically transform categorical variables into dummy codes, which is needed as the glmnet function cannot handle categorical variables
x<-model.matrix(Accept~.,data =  College)[,-1]
y<-Accept
##Note some predictors are highly correlated with each other.
pairs(x, lower.panel=NULL, main="Scatterplots of Predictors")
##alpha=0 for ridge, alpha=1 for LASSO
##threshold value should be very small if multicollinearity is present. see what happens if thresh was set to a larger value
##we know theoretically the coeffs should be the same as lm when lambda is 0
ridge.r<-glmnet(x,y,alpha=0, lambda=0, thresh = 1e-14)
library(ISLR)
attach(College)
library(glmnet)
install.packages("glmnet")
library(ISLR)
attach(College)
library(glmnet)
##model.matrix automatically transform categorical variables into dummy codes, which is needed as the glmnet function cannot handle categorical variables
x<-model.matrix(Accept~.,data =  College)[,-1]
y<-Accept
##Note some predictors are highly correlated with each other.
pairs(x, lower.panel=NULL, main="Scatterplots of Predictors")
##alpha=0 for ridge, alpha=1 for LASSO
##threshold value should be very small if multicollinearity is present. see what happens if thresh was set to a larger value
##we know theoretically the coeffs should be the same as lm when lambda is 0
ridge.r<-glmnet(x,y,alpha=0, lambda=0, thresh = 1e-14)
coefficients(ridge.r)
##MLR
result<-lm(mpg~.,mtcars)
summary(result)
##use CV to find optimal lambda based on training set
set.seed(2019)
cv.out<-cv.glmnet(x[train,],y[train],alpha=0)
##split data
set.seed(2019)
train<-sample(1:nrow(x), nrow(x)/2)
test<-(-train)
y.test<-y[test]
##use CV to find optimal lambda based on training set
set.seed(2019)
cv.out<-cv.glmnet(x[train,],y[train],alpha=0)
bestlam<-cv.out$lambda.min
bestlam
plot(cv.out)
##use CV to find optimal lambda based on training set
set.seed(4630)
cv.out<-cv.glmnet(x[train,],y[train],alpha=0)
bestlam<-cv.out$lambda.min
bestlam
plot(cv.out)
##setup grid for lambdas
grid<-10^seq(10,-2,length=100)
##fit ridge regression using training data
ridge.mod<-glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh = 1e-14)
##Test MSE with best lambda
ridge.pred<-predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
##test MSE with lambda=0
ridge.pred.0<-predict(ridge.mod,s=0,newx=x[test,])
mean((ridge.pred.0-y.test)^2)
##Compare ridge with OLS using best lambda and all observations
out.ridge<-glmnet(x,y,alpha=0,lambda=bestlam,thresh = 1e-14)
out.ols<-glmnet(x,y,alpha=0, lambda=0, thresh = 1e-14)
cbind(coefficients(out.ridge), coefficients(out.ols))
sqrt(sum(coefficients(out.ridge)[-1]^2))
sqrt(sum(coefficients(out.ols)[-1]^2))
##Create plot of ridge coeff against lambda
out.all<-glmnet(x,y,alpha=0,lambda=grid,thresh = 1e-14)
plot(out.all, xvar = "lambda")
abline(v=log(bestlam), lty=2)
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .7)
##use CV to find optimal lambda based on training set
set.seed(4630)
cv.out2<-cv.glmnet(x[train,],y[train],alpha=1)
bestlam2<-cv.out2$lambda.min
bestlam2
plot(cv.out2)
##setup grid for lambdas
grid2<-10^seq(10,-2,length=100)
##fit ridge regression using training data
lasso.mod<-glmnet(x[train,],y[train],alpha=1,lambda=grid, thresh = 1e-14)
##Test MSE with best lambda
lasso.pred<-predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
install.packages("glmnet")
install.packages("glmnet")
##MLR
result<-lm(mpg~.,mtcars)
summary(result)
##MLR
result<-lm(mpg~.,mtcars)
summary(result)
##split data
set.seed(12)
train<-sample(1:nrow(x), nrow(x)/2)
test<-(-train)
y.test<-y[test]
##use CV to find optimal lambda based on training set
set.seed(12)
cv.out<-cv.glmnet(x[train,],y[train],alpha=0)
bestlam<-cv.out$lambda.min
bestlam
plot(cv.out)
library(ISLR)
attach(College)
library(glmnet)
##model.matrix automatically transform categorical variables into dummy codes, which is needed as the glmnet function cannot handle categorical variables
x<-model.matrix(Accept~.,data =  College)[,-1]
y<-Accept
##Note some predictors are highly correlated with each other.
pairs(x, lower.panel=NULL, main="Scatterplots of Predictors")
##alpha=0 for ridge, alpha=1 for LASSO
##threshold value should be very small if multicollinearity is present. see what happens if thresh was set to a larger value
##we know theoretically the coeffs should be the same as lm when lambda is 0
ridge.r<-glmnet(x,y,alpha=0, lambda=0, thresh = 1e-14)
coefficients(ridge.r)
##MLR
result<-lm(Acc~.,mtcars)
summary(result)
##MLR
result<-lm(Accept~.,College)
summary(result)
# Question 2
library(swiss)
# Question 2
attach(swiss)
# Question 2
attach(swiss)
##perform PCA, with scaling.
pr.out<-prcomp(swiss[,c(-8,-9)], scale=TRUE)
##extract the mean and SD of each variable
pr.out$center
pr.out$scale
##same result if you calculated their means and SDs on your own
apply(mtcars[,c(-8,-9)], 2, mean)
apply(mtcars[,c(-8,-9)], 2, sd)
##obtain the loading vector for the PCs
pr.out$rotation
names(mtcars)
# Question 2
attach(swiss)
?swiss
swiss
##perform PCA, with scaling.
pr.out<-prcomp(swiss, scale=TRUE)
##extract the mean and SD of each variable
pr.out$center
pr.out$scale
##same result if you calculated their means and SDs on your own
apply(mtcars[,c(-8,-9)], 2, mean)
apply(mtcars[,c(-8,-9)], 2, sd)
##obtain the loading vector for the PCs
pr.out$rotation
# Question 2
attach(swiss)
?swiss
swiss
##perform PCA, with scaling.
pr.out<-prcomp(swiss, scale=TRUE)
##extract the mean and SD of each variable
pr.out$center
pr.out$scale
##same result if you calculated their means and SDs on your own
apply(mtcars[,c(-8,-9)], 2, mean)
apply(mtcars[,c(-8,-9)], 2, sd)
##obtain the loading vector for the PCs
pr.out$rotation
##SD of each PC
pr.out$sdev
##variance of each PC
pr.var<-pr.out$sdev^2
pr.var
##proportion of variance in data explained by each PC
pve<-pr.var/sum(pr.var)
pve
##plot of first two PCs
biplot(pr.out, scale=0)
##Scree plot
plot(pve, ylim=c(0,1))
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", main="Scree Plot", ylim=c(0,1),type='b')
library(glmnet)
library(glmnet)
library(glmnet)
library(glmnet)
library(glmnet)
library(glmnet)
library(glmnet)
library(glmnet)
##model.matrix automatically transform categorical variables into dummy codes, which is needed as the glmnet function cannot handle categorical variables
x<-model.matrix(mpg~.,mtcars)[,-1]
y<-mtcars$mpg
##Note some predictors are highly correlated with each other.
pairs(x, lower.panel=NULL, main="Scatterplots of Predictors")
##alpha=0 for ridge, alpha=1 for LASSO
##threshold value should be very small if multicollinearity is present. see what happens if thresh was set to a larger value
##we know theoretically the coeffs should be the same as lm when lambda is 0
ridge.r<-glmnet(x,y,alpha=0, lambda=0, thresh = 1e-14)
coefficients(ridge.r)
##MLR
result<-lm(mpg~.,mtcars)
summary(result)
library(ISLR)
attach(College)
library(glmnet)
##model.matrix automatically transform categorical variables into dummy codes, which is needed as the glmnet function cannot handle categorical variables
x<-model.matrix(Accept~.,data =  College)[,-1]
y<-Accept
library(ISLR)
attach(College)
library(glmnet)
##model.matrix automatically transform categorical variables into dummy codes, which is needed as the glmnet function cannot handle categorical variables
x<-model.matrix(Accept~.,data =  College)[,-1]
y<-Accept
##Note some predictors are highly correlated with each other.
pairs(x, lower.panel=NULL, main="Scatterplots of Predictors")
##alpha=0 for ridge, alpha=1 for LASSO
##threshold value should be very small if multicollinearity is present. see what happens if thresh was set to a larger value
##we know theoretically the coeffs should be the same as lm when lambda is 0
ridge.r<-glmnet(x,y,alpha=0, lambda=0, thresh = 1e-14)
coefficients(ridge.r)
##MLR
result<-lm(Accept~.,College)
summary(result)
##split data
set.seed(2019)
train<-sample(1:nrow(x), nrow(x)/2)
test<-(-train)
y.test<-y[test]
##use CV to find optimal lambda based on training set
set.seed(4630)
cv.out<-cv.glmnet(x[train,],y[train],alpha=0)
bestlam<-cv.out$lambda.min
bestlam
plot(cv.out)
##setup grid for lambdas
grid<-10^seq(10,-2,length=100)
##fit ridge regression using training data
ridge.mod<-glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh = 1e-14)
##Test MSE with best lambda
ridge.pred<-predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
##test MSE with lambda=0
ridge.pred.0<-predict(ridge.mod,s=0,newx=x[test,])
mean((ridge.pred.0-y.test)^2)
##Compare ridge with OLS using best lambda and all observations
out.ridge<-glmnet(x,y,alpha=0,lambda=bestlam,thresh = 1e-14)
out.ols<-glmnet(x,y,alpha=0, lambda=0, thresh = 1e-14)
cbind(coefficients(out.ridge), coefficients(out.ols))
sqrt(sum(coefficients(out.ridge)[-1]^2))
sqrt(sum(coefficients(out.ols)[-1]^2))
##Create plot of ridge coeff against lambda
out.all<-glmnet(x,y,alpha=0,lambda=grid,thresh = 1e-14)
plot(out.all, xvar = "lambda")
abline(v=log(bestlam), lty=2)
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .7)
# lasso
##use CV to find optimal lambda based on training set
set.seed(4630)
cv.out2<-cv.glmnet(x[train,],y[train],alpha=1)
bestlam2<-cv.out2$lambda.min
bestlam2
plot(cv.out2)
##setup grid for lambdas
grid2<-10^seq(10,-2,length=100)
##fit ridge regression using training data
lasso.mod<-glmnet(x[train,],y[train],alpha=1,lambda=grid, thresh = 1e-14)
##Test MSE with best lambda
lasso.pred<-predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
##fit OLS by setting lambda=0
ridge.mod.0<-glmnet(x[train,],y[train],alpha=0,lambda=0, thresh = 1e-14)
##test MSE with lambda=0
ridge.pred.0<-predict(ridge.mod.0,newx=x[test,])
mean((ridge.pred.0-y.test)^2)
setwd("C:/Users/jmcarpenter/Desktop/UVA Graduate School Stuff/DataMining/1")
read.csv("College.csv")
college = read.csv("College.csv")
college
rownames(college)
rownames(college)=college[0,1]
fox(college)
fix(college)
college
View(college)
View(college)
rownames(college)
rownames(college)=college[,1]
fix(college)
# Eliminating the names column
college = college[,-1]
fix(college)
# Eliminating the names column
college = college[,-1]
fix(college)
# Eliminating the names column
college = college[,-1]
fix(college)
college = read.csv("College.csv")
rownames(college)=college[,1]
fix(college) # Can confirm there is now a row.names column with the university names going down the column
# Eliminating the names column
college = college[,-1]
fix(college)
college = read.csv("College.csv")
rownames(college)=college[,1]
fix(college) # Can confirm there is now a row.names column with the university names going down the column
# Eliminating the names column
college = college[,-1]
fix(college)
# Use the summary() func to produce a numerical summary of the variables in the data set
summary(college)
# use pairs function to produce a scatterplot matrix
pair(college)
# use pairs function to produce a scatterplot matrix
pairs(college)
# use pairs function to produce a scatterplot matrix
pairs(college[,1:10])
# Use the plot functions to plot outstate vs private
plot(college$Private,college$Outstate)
fix(college)
# Use the plot functions to plot outstate vs private
plot(college$Private,college$Outstate,title="Pr")
# Use the plot functions to plot outstate vs private
plot(college$Private,college$Outstate,main-"Private vs Out of State")
# Use the plot functions to plot outstate vs private
plot(college$Private,college$Outstate,main-"Private vs Out of State")
# Use the plot functions to plot outstate vs private
plot(college$Private,college$Outstate,main="Private vs Out of State")
fix(college)
#
Elite=rep("No",nrow(college))
Elite
fix(Elite)
Elite[college$Top10perc>50]="Yes"
Elite=as.factor(Elite)
college=data.frame(college,Elite)
fix(college)
summary(college)
plot(college$Private,college$Outstate)
# Use hist to produce some histograms
hist(college)
# Use hist to produce some histograms
hist(college$Accept)
# Use hist to produce some histograms
hist(college$Top25perc)
# Use hist to produce some histograms
hist(college$Books)
hist(college$Room.Board)
hist(college$Grad.Rate
hist(college$Grad.Rate)
# Use hist to produce some histograms
hist(college$Books)
hist(college$Room.Board)
hist(college$Grad.Rate)
par(mfrow=(2,2))
par(mfrow=c(2,2))
hist(college$Books)
hist(college$Room.Board)
hist(college$Grad.Rate)
# Continue exploring the data and see what you find
Elite
# Continue exploring the data and see what you find
college
# Continue exploring the data and see what you find
college$Elite
# Continue exploring the data and see what you find
fix(college)
